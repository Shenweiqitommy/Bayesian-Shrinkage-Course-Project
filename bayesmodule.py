# -*- coding: utf-8 -*-
"""BayesModule

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IYxZFyheYefHOTXNyuRf0ABq34DyZ6dI
"""

import numpy as np
import pymc as pm
import arviz as az

class BayesianShrinkageRegression:
    def __init__(self, prior='gaussian', draws=2000, tune=1000, target_accept=0.95):
        """
        Bayesian Linear Regression with shrinkage priors.
        prior: 'gaussian', 'laplace', or 'horseshoe'
        draws, tune: MCMC sampling iterations and burn-in
        target_accept: NUTS sampler target acceptance probability
        """
        assert prior in ['gaussian', 'laplace', 'horseshoe'], "Invalid prior choice"
        self.prior = prior
        self.draws = draws
        self.tune = tune
        self.target_accept = target_accept
        self.model = None
        self.idata = None          # ArviZ InferenceData after fitting
        self.X_train = None
        self.y_train = None

    def fit(self, X, y):
        """Build and sample the Bayesian regression model given design matrix X and target y."""
        X = np.asarray(X)
        y = np.asarray(y)
        # Store training data for diagnostics and future use
        self.X_train = X
        self.y_train = y
        n_samples, n_features = X.shape
        # Define model with coordinate names for observations and features
        coords = {"obs_id": np.arange(len(y)), "feature_id": np.arange(n_features)}
        with pm.Model(coords=coords, coords_mutable={"obs_id": np.arange(len(y))}) as self.model:
            # Mutable data container for predictors (allows later updating for prediction)&#8203;:contentReference[oaicite:3]{index=3}
            X_data = pm.MutableData("X", X, dims=("obs_id", "feature_id"))
            # Intercept with a diffuse prior
            intercept = pm.Normal("Intercept", mu=0, sigma=10.0)
            # Coefficient priors based on chosen shrinkage prior
            if self.prior == 'gaussian':
                beta = pm.Normal("beta", mu=0, sigma=1.0, dims=("feature_id",))
            elif self.prior == 'laplace':
                beta = pm.Laplace("beta", mu=0, b=1.0, dims=("feature_id",))
            elif self.prior == 'horseshoe':
                # Horseshoe prior: global tau and local lambda (half-Cauchy), non-centered parameter z
                tau = pm.HalfCauchy("tau", beta=1.0)
                lam = pm.HalfCauchy("lambda", beta=1.0, dims=("feature_id",))
                z = pm.Normal("z", mu=0, sigma=1.0, dims=("feature_id",))
                beta = pm.Deterministic("beta", z * tau * lam, dims=("feature_id",))
            # Noise standard deviation prior (weakly informative)
            sigma = pm.HalfNormal("sigma", sigma=1.0)
            # Linear model for the mean of y
            mu = intercept + pm.math.dot(X_data, beta)
            # Likelihood (observed y)
            pm.Normal("y_obs", mu=mu, sigma=sigma, observed=y, dims=("obs_id",))
            # Sample from the posterior
            self.idata = pm.sample(draws=self.draws, tune=self.tune,
                                    target_accept=self.target_accept,
                                    return_inferencedata=True, progressbar=False)
        # Check convergence diagnostics
        summary_df = az.summary(self.idata, round_to=2)
        max_rhat = summary_df['r_hat'].max()
        min_ess = summary_df[['ess_bulk', 'ess_tail']].min().min()
        if max_rhat > 1.05 or min_ess < 500:
            print(f"WARNING: Potential convergence issues (max r_hat={max_rhat:.2f}, min ESS={min_ess:.0f}).")
        return self

    def summary(self):
        """Return a dataframe with posterior summary statistics (mean, sd, hdi, R-hat, ESS, etc.)."""
        if self.idata is None:
            raise RuntimeError("No inference data available. Fit the model first.")
        return az.summary(self.idata)

    def plot_trace(self):
        """Plot trace of posterior samples for key parameters (Intercept, beta coefficients, sigma)."""
        if self.idata is None:
            raise RuntimeError("No inference data - fit the model first.")
        return az.plot_trace(self.idata, var_names=["Intercept", "beta", "sigma"], combined=True)

    def predict(self, X_new):
        """Generate posterior predictive samples for new input data X_new."""
        if self.idata is None or self.model is None:
            raise RuntimeError("Call fit() before predict().")
        X_new = np.asarray(X_new)
        with self.model:
            # Update the shared data container "X" with new data and provide new coordinates
            pm.set_data({"X": X_new}, model=self.model, coords={"obs_id": np.arange(X_new.shape[0])})
            ppc = pm.sample_posterior_predictive(
                self.idata, var_names=["y_obs"], return_inferencedata=False,
                progressbar=False, predictions=True
            )
            # Reset "X" back to training data (with original coords)
            if self.X_train is not None:
                try:
                    pm.set_data({"X": self.X_train}, model=self.model, coords={"obs_id": np.arange(self.X_train.shape[0])})
                except Exception:
                    pass
        y_preds = np.array(ppc["y_obs"])
        if y_preds.ndim == 3:
            chains, draws, n_obs = y_preds.shape
            y_preds = y_preds.reshape(chains * draws, n_obs)
        return y_preds  # shape (n_posterior_samples, n_new_samples)



    def plot_residuals(self, ax=None):
        """
        Plot residuals vs fitted values for training data.
        Residual = observed y - posterior mean prediction for each sample.
        """
        if self.idata is None or self.model is None:
            raise RuntimeError("Model is not yet fitted.")
        if self.X_train is None or self.y_train is None:
            raise RuntimeError("Training data not stored.")
        # Ensure model uses training data for prediction
        try:
            self.model.set_data({"X": self.X_train})
        except Exception:
            pass
        # Posterior predictive draws for training data
        with self.model:
            ppc = pm.sample_posterior_predictive(
                self.idata, var_names=["y_obs"], return_inferencedata=False, progressbar=False
            )
        y_pred = np.array(ppc["y_obs"])
        if y_pred.ndim == 3:  # flatten chain/draw if needed
            y_pred = y_pred.reshape(-1, y_pred.shape[-1])
        pred_mean = y_pred.mean(axis=0)               # posterior mean predictions per sample
        residuals = self.y_train - pred_mean          # residuals per sample
        # Plot residuals
        import matplotlib.pyplot as plt
        if ax is None:
            fig, ax = plt.subplots(figsize=(6, 4))
        ax.scatter(pred_mean, residuals, alpha=0.7)
        ax.axhline(0, color='gray', linestyle='--', linewidth=1)
        ax.set_xlabel("Fitted values")
        ax.set_ylabel("Residuals (observed - fitted)")
        ax.set_title("Residuals vs Fitted")
        return ax